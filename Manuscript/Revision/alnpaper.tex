\documentclass[11pt]{article}

\usepackage[margin=1.0in]{geometry}
%\linespread{1.5}
\usepackage{graphicx}
\usepackage{natbib}

\pdfminorversion 4

\bibpunct[,]{(}{)}{;}{a}{}{,}
%\usepackage{tablefootnote}
\usepackage{amsmath}
\sloppy

\renewcommand{\bottomfraction}{.9}
\renewcommand{\topfraction}{.9}
\renewcommand{\textfraction}{0.1}
\renewcommand{\floatpagefraction}{.9}


\usepackage{fancyhdr}

\fancypagestyle{plain}{%
   \fancyhead[R]{\fbox{\big{\textbf{Letter}}}}
   \renewcommand{\headrulewidth}{0pt}
}


\begin{document}

\title{\textbf{Limited utility of residue masking for positive-selection inference}}
\author{Stephanie J. Spielman$^{1*}$ and Eric T. Dawson$^{1}$ and Claus O. Wilke$^{1}$}
\date{}

\maketitle
\noindent
Address:\\
$^1$Department of Integrative Biology, Center for Computational Biology and Bioinformatics, and Institute of Cellular and Molecular Biology.
The University of Texas at Austin, Austin, TX 78712, USA.\\

\bigskip
\noindent
$^*$Corresponding author\\
$\phantom{^*}$Email: stephanie.spielman@utexas.edu\\

\bigskip
\noindent
Manuscript type: Letter

\bigskip
\noindent Keywords: multiple sequence alignment, alignment filters, positive-selection inference, sequence simulation

\newpage
\begin{abstract}
Errors in multiple sequence alignments (MSAs) are known to reduce accuracy in positive-selection inference. Thus, it has been suggested that users filter MSAs before conducting further analyses. One such widely-used filter, Guidance, generates site-specific MSA confidence scores, allowing users to remove positions of low confidence. Studies investigating this filter's utility for positive-selection inference have yielded inconsistent results; some have demonstrated that Guidance substantially improved accuracy, but others have found that Guidance affected accuracy very minimally. Motivated by these discrepancies, we have conducted a extensive simulation-based study to fully characterize how Guidance impacts positive-selection inference for realistic protein-coding sequences. We particularly investigated whether novel scoring algorithms, which phylogenetically correct confidence scores, or a new gap-penalization score-normalization scheme could improve upon Guidance's performance. We found that no filter, including the original Guidance, substantially improved positive-selection inference across multiple inference methods. Instead, the analysis method used influenced positive-selection inference far more than did alignment filtering.
% REWRITE ABSTRACT WHEN NEW MS IS DONE BECAUSE WE ARE ADDING IN QQCH ABOUT THE ROUTE OF POWER UPTICK.
\end{abstract}


\section*{Introduction}
Constructing a multiple sequence alignment (MSA) represents the first step of analysis in most studies of molecular evolution, including phylogenetic reconstruction and evolutionary rate inference. Recently, several studies have shown that poor MSA quality can significantly hinder accuracy in positive-selection inference  \citep{Schneider2009, Fletcher2010, MarkovaRaina2011}. As a consequence, some studies have advocated that users filter MSAs before subsequent analyses to remove putatively-poorly aligned regions \citep{Jordan2012,Privman2012}. This strategy, in theory, will maximize signal without excessively sacrificing power.

% probably/definitely cut this paragraph down even more.
One filter, known as Guidance \citep{Penn2010}, is widely used in positive-selection inference. Guidance derives a confidence score for each MSA position by sampling variants in the guide tree used to construct progressive alignments. Using these confidence scores, users can mask positions that score below a set threshold, thereby removing residues that cannot be confidently aligned. Unfortunately, studies investigating Guidance's utility in positive-selection inference have produced seemingly conflicting results. While one study by \citet{Privman2012} found that Guidance had the ability to dramatically dramatically reduce false positive rates, a separate study by \citet{Jordan2012} found that Guidance affected positive-selection inference modestly, if at all. 
In particular, \citet{Privman2012} and \citet{Jordan2012} concluded, respectively, that Guidance improved inference most substantially at high insertion/deletion (indel) rates and/or at high divergence levels. However, it is important to recognize that protein-coding sequences typical of positive-selection inference studies rarely contain sequences with those indel rates or divergence levels.

To reconcile these different findings, we have conducted an extensive simulation-based study to fully elucidate how the Guidance filter influences positive-selection inference. In particular, we examined the potential benefits to modifying the Guidance scoring scheme in several ways. First, we assessed whether novel algorithms that correct Guidance scores for the sequences' phylogenetic relationships could improve upon the original Guidance algorithm. Second, we tested a new score-normalization scheme which scaled residue scores according to the number of gaps in its column; this strategy naturally assigned lower scores to residues in ``gappy" columns, thereby capturing the inherent unreliability of such regions. 

% PROBABLY REWRITE THIS CHUNK.
Overall, we found that neither the original Guidance filter nor our newly implemented filters conferred any substantial benefits to positive-selection inference. In cases in which filtering with a Guidance-based strategy did benefit inference, improvements recovered were of extremely small magnitude. \textbf{this sentence is awful: Moreover, we have found that the modest boosts in power that Guidance-based filtering can provide seem to occur due to resulting changes in the statistical properties of the inference method.} 


\section*{Results}

\subsection*{Guidance reimplementation and analysis pipeline}
To systematically evaluate Guidance's influence on positive-selection inference, we reimplemented the Guidance software in Python and C++ (available at https://github.com/clauswilke/alignment\underline{\hspace*{0.2cm}}filtering). Before conducting any analyses, we carefully verified that, given a set of perturbed alignments, our program produced the same confidence scores as did the original Guidance. This reimplementation was necessary for us to investigate the utility of several novel scoring schemes, described in detail in Methods. These new methods included two scoring algorithms which incorporated phylogenetic weights to assign confidence scores. Briefly, the first method incorporated a weight for each sequence in the alignment, as calculated by BranchManager \citep{Stone2007}, and the second method incorporated patristic distances (the sum of branch lengths between two taxa). We called these methods, respectively, BMweights and PDweights. Moreover, we defined a new ``gap-penalization" score-normalization scheme, which assigned scores scaled by a column's gappiness. Specifically, this scheme naturally gave lower scores to highly-gapped columns, accounting for the fact that such regions were more likely to be poorly aligned. We referred to filters using the gap-penalization scheme as GuidanceP, BMweightsP, and PDweightsP.

We simulated 100 replicates of protein-coding sequences using Indelible \citep{Fletcher2009} by evolving sequences along each of four different gene trees of sizes 11, 26, 60, and 158 taxa. We simulated sequences according to two distinct selective profiles: H1N1 influenza hemagluttinin (HA) and HIV-1 envelope protein subunit GP41 (see SI for details). The HA selective profile had a mean $dN/dS = 0.37$, and the GP41 profile had a mean $dN/dS = 0.89$. In other words, the majority of sites in the HA $dN/dS$ distribution were either under strong purifying or positive selection, but the GP41 $dN/dS$ distribution featured a much larger proportion of sites near neutral, whose evolutionary rates should be more difficult to infer.  

We processed the unaligned amino-acid sequences with our Guidance reimplementation using the aligner MAFFT L-INS-I (linsi) \citep{Katoh2005}. We calculated confidence scores for all inferred MSAs using each of six scoring algorithms. Unless otherwise specified, we masked positions with scores below 0.5, the same threshold as used by \citet{Jordan2012}. After back-translating these protein MSAs to codon sequences using the original nucleotide data, we assessed positive selection with both the recently-described FUBAR \citep{Murrell2013}, as implemented in HyPhy \citep{Pond2005}, and the widely-used M8 model in PAML \citep{Yang2007}. We considered sites to be positively selected if the given method returned a posterior probability $\geq0.90$. Note that while we processed all MSAs with FUBAR, we did not infer positive selection with PAML for simulations of 158 taxa due to prohibitive runtimes. 

\subsection*{Guidance-based filters have a minimal effect on positive-selection inference}

We assessed how filtering MSAs with a Guidance-based method influenced positive-selection detection by comparing the resulting true positive rates (TPRs) of positive-selection inference between all filtered MSAs and their corresponding unfiltered MSAs. TPRs were calculated using the true evolutionary rates assigned during sequence simulation. We chose to primarily compare the TPRs as opposed to the false positive rates (FPRs), as the FPRs we recovered were exceedingly small, never exceeding an average of 0.005 across simulation sets and methods, similar to those recovered by \citet{Jordan2012}. 

To assess performances of our different Guidance-based filters, we fit a series of mixed-effects models, which included TPR as the response variable, filtering algorithm as a fixed effect, and simulation count as a random effect (to account for the paired structure of our analysis). Table \ref{tab:casemodel} highlights key findings from these models (see Tables S1, S2 for complete results). We found that, for all but one simulation set, there was no significant difference among filter performance within a given normalization scheme. In other words, Guidance, BMweights, and PDweights yielded comparable performances, as did GuidanceP, BMweightsP, and PDweightsP. Therefore, Table \ref{tab:casemodel} displays results for only Guidance and GuidanceP. 

Results from FUBAR and PAML yielded largely comparable results. In all cases in which MSA filtering improved mean TPR, the improvement was exceedingly minimal. For instance, the most substantial TPR improvement recovered was for the simulation set according to the HA selective profile with 26 sequences; PAML yielded roughly a 4\% TPR increase, relative to the unfiltered MSA, with the GuidanceP filter. However, when positive selection was inferred with FUBAR, no significant TPR change was detected. To complicate matters further, while the GuidanceP filter, in this case, provided the largest benefit seen here, for many other simulation sets this filter actually worsened positive-selection inference. Therefore, filtering MSAs with a Guidance-based approach modestly improved positive-selection inferences in certain circumstances, it hindered accuracy in others. It is, however, important to note that, for both simulation sets of 158 sequences, filtering universally provided a significant TPR boost, albeit by only roughly 2\%.

We used receiver operating characteristic (ROC) curves to qualitatively assess differences in positive-selection inference for unfiltered versus filtered alignments. Commonly used to evaluate the performance of binary classification methods, ROC illustrate the relationship between TPR and FPR. Classifier performances can be assessed by comparing area under the ROC curves, such that larger areas indicate higher power. Here, we used ROC curves to examine if alignment filtering helped FUBAR and PAML to classify sites more accurately as positively selected or not (Figure \ref{roc}). We broadly found that, when you look at the entire curve, no difference is really there. When we instead zoom into the low FPR region, we can see what appear to be substantial benefits. This does not conflict with our earlier stuff, because remember, we found very low FPR rates in our simulations. Thus, the region where our simulations found results was at that very tiny chunk where no difference is really apparent. It therefore seems that, under conditions with relatively higher FPR (2-4\%), filtering may help. However, as the figure shows, this benefit mostly dissipates by a FPR of 5\%. 

So, we are now confronted with the question - there is a seeming boost of power here. Where is this improved power coming from, given that information is merely removed, which should, in theory, be less power since some data was removed? We suggest that any benefits of filtering arose from the missing data directly influencing the statistical behavior of the inference methods, FUBAR and PAML. We examined the MLE for the dN/dS>1 category for PAML, and for FUBAR we determined the weighted mean dN/dS value for those grid points which were greater than 1. We then ran a second set of mixed-effects models, for which this omega value was the response, algorithms (including only unfiltered, Guidance, GuidanceP) were fixed effects, and sim count was a random effect. Intriguingly, we found that, for nearly all simulation conditions in which we recovered significant TPR boosts, the omega was significantly larger for the filtered MSA than the unfiltered MSA. The primary exception to this was HA 158, for which omega was basically the same. Even so, this trend suggests that inference methods respond in bizarre ways to more missing data which, oddly, moderately boosts TPR...
We have shown that this exists for PAML and FUBAR, so if you are going to filter with a different inference method, we suggest not to. Because I mean, you don't know, right?!

Moreover, we double-checked the masking situation, and found that fifty was a better idea because for GuidanceP, ninety was often a huge problem.

\subsection*{Influence of posterior probability on selection inference methods}

Our analyses incidentally recovered several differences between how PAML and FUBAR behaved when assessing positive selection. In particular, whether FUBAR or PAML performed better depended on the number of sequences analyzed; FUBAR outperformed PAML for the two smaller simulation sets of 11 and 26 sequences, but PAML outperformed FUBAR for the 60-sequence simulation set (Table \ref{tab:casemodel}). We found that this trend resulted from the posterior probability threshold chosen to call sites as positively selected. Figure \ref{tprfpr} shows how the use of different posterior probabilities affected the mean TPR and FPR for unfiltered alignments from the 26- and 60-sequence simulation sets. 

With regards to TPR, FUBAR generally outperformed PAML at low posterior probabilities, whereas PAML outperformed FUBAR at high probabilities. As the number of sequences increased, the intersection between methods' TPRs shifted towards lower posterior probabilities. This shift was largely dictated by PAML's improved ability to call true positives with the inclusion of more sequences, given that FUBAR's relationship between posterior probability and TPR remained similar between the two simulation sets.  Filtering alignments affected these broad trends that FUBAR and PAML portrayed only marginally, if at all (Figure \ref{fulltpr}). These results demonstrated that the analysis method used affected positive-selection inference substantially more than did filtering alignments with a Guidance-based method. 

The relationship between FPR and posterior probability was largely consistent between simulation sets for both FUBAR and PAML. While PAML achieved mean FPRs of nearly zero essentially immediately for both the 26 and 60-sequence simulation sets, FUBAR approached a zero mean FPR more slowly, reaching zero around a posterior probability of 0.8 for each simulation set. As a typical study of positive selection would almost certainly use a posterior probability above at least 0.8, alignment filtering would likely not be able to substantially reduce FPR.

How TPR and FPR behaved for FUBAR and PAML evidenced the different approaches these methods employ to detect positive selection. PAML's M8 model employs a random-effects likelihood (REL) method \citep{NielsenYang1998} to fit a specific probabilistic model of sequence evolution. FUBAR, on the other hand, approximates results one would achieve with an REL method, allowing for remarkably fast runtimes without excessively sacrificing performance. Having fewer sequences in an alignment hindered PAML's ability to precisely fit its given model, rendering its positive-selection inference worse than that of FUBAR. Increasing the number of sequences allowed PAML to achieve a more accurate model fit, and in turn positive-selection inferences, but had a relatively smaller effect on FUBAR's approximations.

\subsection*{Raising masking thresholds for gap-penalization algorithms can hinder positive-selection inference}

When filtering alignments with Guidance-based methods, one must select a specific score cutoff below which to mask residues. We chose to filter all residues with scores less than 0.5, as previously done by \citet{Jordan2012}. However, it was possible that selecting a different threshold would have yielded different results, so we analyzed how changing this threshold might impact our findings. For this analysis, we considered only the Guidance and GuidanceP scoring schemes since our phylogenetically-corrected algorithms did not perform significantly differently. Using the same position confidence scores previously generated, we masked all alignments at the additional cutoffs of 0.3, 0.7, and 0.9 and inferred positive selection with FUBAR and PAML.

To investigate the influence of different masking cutoffs, we fit mixed-effects linear models for each simulation set, with TPR as the response, masking cutoff as a fixed effect, and simulation count as the random effect, for Guidance and GuidanceP results each. 
Results showed that, when analyzed with FUBAR, Guidance alignments, for a given simulation set, yielded statistically similar TPRs (all $P>0.06$) across masking thresholds. When analyzed with PAML, there was no significant TPR difference among masking thresholds for both the 11- and 26-sequence simulation sets (all $P>0.10$). PAML's results for the 60-sequence simulation set, however, demonstrated that masking at a threshold of 0.9 was significantly worse than masking at thresholds of 0.3 or 0.5 (each comparison gave $P<0.001$), although there was no significant difference between the 0.3 and 0.5 thresholds' TPRs ($P=0.65$). Masking at 0.9, in fact, resulted in roughly a 4\% decrease in TPR, relative to masking at 0.5. This result demonstrated that employing a stringent masking threshold may hinder accuracy in positive-selection inference under certain conditions.

Indeed, we found that, for alignments masked using the GuidanceP algorithm, while the masking cutoffs of 0.3, 0.5, and 0.7 yielded statistically similar TPRs (all $P>0.45$), masking at the 0.9 threshold universally produced significantly lower TPRs than did the other cutoffs. This trend held for both FUBAR and PAML, as highlighted in Table \ref{tab:cutoffs}, which specifically compares performance between cutoffs 0.5 and 0.9 for GuidanceP. Importantly, for the 60-sequence simulation set, masking at the stringent cutoff of 0.9, as opposed to the more lenient 0.5, reduced the average TPR by over 25\%, when analyzed by both FUBAR and PAML. For both positive-selection inference methods, these extreme TPR decreases produced TPRs well below their corresponding unfiltered TPRs (Table \ref{tab:casemodel}). When combined with a strict masking cutoff, the gap-penalization normalization scheme, therefore, had the potential to remove excessive amounts of information from alignments and worsen positive-selection detection. 


\section*{Discussion}

We have conducted an extensive simulation-based study to evaluate how introducing phylogenetically-aware scoring algorithms into the Guidance alignment filter influenced the detection of positive selection in protein-coding sequences. Additionally, we tested a novel gap-penalization score-normalization method, which scaled residue confidence scores according to the number of gaps in that residue's column. Using coding-sequence data simulated according to evolutionary parameters of the H1N1 influenza hemagluttinin (HA) protein, we inferred positive selection for all alignments using two methods: the recently introduced and very fast FUBAR \citep{Murrell2013} within the HyPhy package \citep{Pond2005} and the current standard for evolutionary rate inference, the PAML M8 model \citep{Yang2007}, totaling over 9000 positive-selection inferences.

We found that, while the gap-penalization scheme marginally improved upon the original normalization method, incorporating phylogenetic information did not significantly impact positive-selection inference relative to the original Guidance algorithm. Moreover, filtering alignments with any Guidance-based method, including the original, did not substantially improve positive-selection inference under any circumstance. That the phylogenetically-weighted algorithms did not improve upon the original Guidance algorithm indicated the minimal benefits that filtering in this manner produced at all. Were the original Guidance to offer robust improvements in positive-selection detection, one might expect that our more statistically controlled approach would boost the method's performance. However, as we have found that masking individual positions in an alignment only marginally affected positive-selection inference in the first place, the algorithmic changes we implemented would not be expected to have a dramatic effect.

We ensured realism in our simulations by applying evolutionary parameters inferred from a very large (1038 sequences) HA alignment. We chose to use HA as a reference for evolutionary parameters for two reasons; its evolutionary history is marked by pervasive positive selection \citep{Bush1999, Kryazhimskiy2008, Meyer2012}, and it has a wealth of sequence data. In particular, having such a large data set allowed for robust evolutionary inference and the assurance that our simulation parameters mimicked realistic sequence data. However, rather than conduct our simulations along our HA phylogeny, we decided to evolve sequences along eukaryotic gene trees for tractability. Simulating along a phylogeny of over 1000 sequences would not only represent an excessive computational burden, but also a typical positive-selection study would likely not use this many sequences. Additionally, the most important aspect to consider when selecting a phylogeny along which to simulate sequences is the phylogeny's topology, and the gene trees we used were topologically typical of an evolutionary rate study.

While others \citep{Schneider2009, Fletcher2010, MarkovaRaina2011,Privman2012} have noted a prevalence of false positives in positive-selection inference, we recovered very low ($<2\%$, on average) FPRs, very similar to those found by \citet{Jordan2012}, yet substantially lower than those detected by \citet{Privman2012}, the two studies which have previously investigated the utility of the Guidance filter in positive-selection inference. The high FPRs \citet{Privman2012} recovered likely resulted from the overly high indel rates with which they simulated sequences. Our focus on realistic indel rates and divergence levels supported the conclusions made by \citet{Jordan2012}, namely that the Guidance filter does not substantially increase accuracy in positive-selection inference.

Both \citet{Privman2012} and \citet{Jordan2012} found that filtering alignments with Guidance yielded fewer benefits when alignments were built with a high-quality aligner, namely, PRANK \citep{Loytynoja2008}, as opposed to with other alignment softwares like ClustalW \citep{Thompson1994} or MUSCLE \citep{Edgar2004}. While PRANK has been shown to substantially reduce alignment errors \citep{Loytynoja2008,Privman2012,Jordan2012}, it carries a rather long runtime that may not be practical for some users. To circumvent this time constraint, we conducted all alignments in the present study with linsi, an algorithm released with the MAFFT software that is substantially more accurate than MAFFT's default algorithm \citep{Katoh2005, Nuin2006, Thompson2011} but retains MAFFT's rapid runtime. Importantly, neither previous study investigated how alignment filtering affected alignments made with linsi. We have demonstrated that alignment filtering did not largely increase accuracy in positive-selection inference, similar to how alignment filtering impacted PRANK alignments. Linsi, therefore, may represent a fast and accurate alternative to PRANK.

Our study incidentally recovered some differences in how FUBAR and PAML's M8 model behave when inferring positive selection. Specifically, we noted distinct behaviors of posterior probability thresholds for each method. Posterior probabilities for FUBAR behaved largely as expected; as the threshold became more stringent, fewer positive results were detected. PAML, on the other hand, was fairly robust to the choice of posterior probability threshold; its FPR was nearly zero even at the lowest posterior probabilities, and increasing the threshold's stringency did not strongly affect the TPR. Thus, while using a higher posterior probability in FUBAR may protect against recovering false positives, choosing a higher posterior probability did not similarly affect PAML. Lower posterior probabilities may not severely hinder accuracy in PAML's M8 model.

Moreover, PAML outperformed FUBAR when processing larger sequence sets. However, this increase in performance was coupled with a dramatic increase in runtime. While a single PAML took roughly an hour for each 11-sequence alignment, it took anywhere from 2 days to a week to infer evolutionary rates for a 60-sequence alignment. FUBAR, on the other hand, consistently required under 20 minutes minutes to analyze an alignment of any size and, while not as accurate as PAML for larger data sets, did perform fairly well. Users, therefore, should consider the trade-off between accuracy and runtime when selecting an inference software. 

In sum, we have found that, while alignment filtering offered some benefits to positive-selection inference, those improvements were marginal at best. With such a minimal effect, alignment filtering could easily decrease accuracy in a given positive selection study. Indeed, we noted that using a stringent masking cutoff of 0.9 for algorithms normalized with our gap-penalization strategy, or with a large sequence set, resulted in extreme decreases in TPR relative to an unfiltered alignment. Choosing a low filtering threshold was necessary to achieve any improvement in positive-selection inference.  

Overall, we cannot unequivocally recommend the use of a Guidance-based alignment filter when inferring positive selection. Once an alignment has been constructed, it does not seem that much can be done to eliminate any misleading information. Instead, users should employ inference methods in which the error can be minimized as much as possible without necessitating post-hoc correction. Therefore, we recommend that users select high-quality alignment and inference methods to minimize any obscuring signal, instead of relying on filters. If one must filter an alignment, we recommend using a lenient cutoff ($\leq0.5$) to avoid sacrificing power, which might worsen inferences.


\section*{Materials and Methods}

\subsection*{Guidance Reimplementation}
Our reimplemented Guidance is written in Python and C++. Following the algorithm set forth in Penn et al. \citep{Penn2010}, we first create a reference alignment using a user-specified progressive alignment software, with choices of Clustalw \citep{Thompson1994}, MUSCLE \citep{Edgar2004}, or MAFFT \citep{Katoh2002, Katoh2005}. We then generate $N$ (where $N=100$, by default) bootstrapped alignment replicates, each of which is used to create a bootstrapped tree in FastTree2 \citep{Price2010}. We then use these $N$ trees as guide trees to create $N$ new perturbed alignments, which we subsequently compare to the reference alignment to generate a confidence score for each residue. Users can specify options for their aligner and phylogeny reconstruction method as desired.

\subsubsection*{Scoring Algorithms}
Before calculating confidence scores, a phylogeny is built from the reference alignment. Our program includes functionality to build this phylogeny using either FastTree2 \citep{Price2010} or RAxML \citep{Stamatakis2006}. Two types of phylogenetic weights can be calculated from this tree. The first uses the software package BranchManager \citep{Stone2007} to calculate a weight for each taxon in the phylogeny representing that taxon's contribution to the phylogeny as a whole. We call this method ``BMweights." The second method calculates patristic distances (sum of branch lengths) between each taxon in the phylogeny using the python package DendroPy \citep{Sukumaran2010}. We call this method ``PDweights."

We calculate positional confidence scores for each of the $N$ bootstrap alignments as follows. A raw score, $S_{ij}$, for a given residue in row $i$, column $j$ of the reference alignment is calculated as \begin{equation} S_{ij} = \sum\limits_{k \in R_\text{ng}^{(j)}} I_{ik}^{(j)} s_{ik}   ,\end{equation} where $R_\text{ng}^{(j)}$ represents the set of rows in column $j$ which are not gaps. We calculate $s_{ik}$ according to the given scoring algorithm:
\begin{equation}
s_{ik}} = \left\{ \begin{array}{rl}

              1                         &\mbox{if Guidance} \\
              w_iw_k              &\mbox{if BMweights} \\
              d_p(i,k)              &\mbox{if PDweights} \\
                     \end{array} \right.,
\end{equation} where $w_i$ is the phylogenetic weight of the taxon at row $i$, as calculated by BranchManager, and $d_p(i, k)$ is the patristic distance between the taxa at rows $i$ and $k$. 
The indicator function 
\begin{equation}I_{ik}^{(j)} = \left\{ \begin{array}{rl}

              1                         &\mbox{if reference alignment residue pair $(i, k)^{(j)}$ is present in bootstrap alignment} \\
              0            &\mbox{if reference alignment residue pair $(i, k)^{(j)}$ is absent in bootstrap alignment} \\
                     \end{array} \right. 
\end{equation}
serves to compare the bootstrap- and reference-alignment residue pairings.


We then sum positional scores $S_{ij}$ determined from each bootstrap replicate $n$. We normalize these scores across bootstrap replicates to yield a final score $\widetilde{S}_{ij}$ for each residue in the reference alignment. We use two different normalization schemes: original Guidance (defined in \citet{Penn2010}) and a novel gap-penalization scheme. These normalization schemes are given by \begin{equation}
\widetilde{S}_{ij}  = \sum_n S_{ij}(n) \bigg{/} \left\{ \begin{array}{rl}

              \sum\limits_n \sum\limits_{k \in R_\text{ng}^{(j)}} s_{ik}(n)     &\mbox{if original Guidance} \\
              \sum\limits_n \sum\limits_{k \in R_\text{all}^{(j)}} s_{ik}(n)     &\mbox{if gap-penalization} \\      
        \end{array} \right.,
\end{equation} 
where $R_\text{all}^{(j)}$ represents all rows in column $j$, including gaps, the sums over $n$ run over all $N$ replicates, and $S_{ij}(n)$ and $s_{ik}(n)$ each represent those respective quantities for bootstrap replicate $n$. By considering all rows instead of just rows that are not gaps, the gap-penalization scheme will naturally assign lower scores to highly gapped columns. We refer to the algorithms normalized by the original Guidance scheme as Guidance, BMweights, and PDweights. When normalized with the gap-penalization scheme, we refer to them, respectively, as GuidanceP, BMweightsP, and PDweightsP. Note that scores calculated using the Guidance algorithm with the original normalization scheme are equivalent to those originally derived by \citet{Penn2010}. 



\subsection*{Sequence Simulation}
Coding sequences were simulated using Indelible \citep{Fletcher2009}. To ensure that our simulations reflected realistic protein sequences, we simulated according to evolutionary parameters of the H1N1 hemagluttinin (HA) influenza protein. To derive these parameters, we aligned 1038 HA protein sequences collected from the Influenza Research Database (http://www.fludb.org) with MAFFT, specifying the ``--auto" flag, \citep{Katoh2002,Katoh2005} and then back-translated to a codon alignment using the original nucleotide sequence data. We generated a phylogeny from this codon alignment in RAxML \citep{Stamatakis2006} using the GTRGAMMA model. Using the codon alignment and phylogeny, we inferred evolutionary parameters with the REL (random effects likelihood)  method \citep{NielsenYang1998} using the HyPhy software \citep{Pond2005}, with five $dN/dS$ rate categories as free parameters under the GY94 evolutionary model \citep{GoldmanYang1994}. We employed a Bayes Empirical Bayes approach \citep{Yang2000} to obtain infer $dN/dS$ values at each site, which we used to assess a complete distribution of site rates. The resulting distribution was log-normal with a mean $dN/dS = 0.37$ with 8.3\% of sites  under positive selection ($dN/dS>1$). We binned these rates into 50 equally spaced categories for specification in Indelible, which required a discrete distribution of $dN/dS$ values. Again according to parameters derived from the HA analysis, we fixed $\kappa$, the transition-to-transversion ratio, at 5.3 for all simulations. We additionally set the average alignment length to 400 codons with state codon frequencies equal to those directly calculated from the HA alignment.

We simulated 100 alignments across four different real gene trees each, yielding a total of 400 simulated alignments. Phylogenies used included an 11-taxon tree of the mammalian olfactory receptor OR5AP2 \citep{Spielman2013}, a 26-taxon tree of mammalian rhodopsin sequences \citep{Spielman2013}, a 60-sequence tree of phosphoribulokinase (PRK) genes from photosynthetic eukaryotes \citep{Yang2011}, and a 158-taxon multilocus tree of flatfish sequences \citep{Betancur2013}. The latter two phylogenies were obtained from TreeBASE (http://treebase.org).
We set all insertion-deletion (indel) rates at 0.05, motivated by studies demonstrating that indel events occur at a rate 5\% of the substitution rate in mammalian genomes \citep{Cooper2004}.

\subsection*{Alignment and Positive-Selection Inference}

We constructed all alignments using linsi \citep{Katoh2002,Katoh2005} within the context of our Guidance reimplementation. Phylogenies used to calculate phylogenetic weights for the BMweights and PDweights algorithms were constructed in RAxML, specifying ``-m PROTCATWAG" as the model of sequence evolution \citep{Stamatakis2006,Stamatakis2006C}. In addition to an unfiltered alignment, we generated six filtered alignments (one for each filtering algorithm and each normalization scheme), masking residues with scores $\leq0.5$ with ``?". To investigate potential biases introduced by the scoring threshold, we also masked residues below the scoring cutoffs of 0.3, 0.7, and 0.9 for alignments constructed with the Guidance and GuidanceP filters.

We inferred positive selection using both FUBAR \citep{Murrell2013} with default parameters and using PAML's M8 model, specifying F3x4 codon frequency and ``cleandata = 0" in the control file \citep{Yang2007}. Phylogenies specified for positive-selection inference were those constructed during the Guidance alignment procedure when deriving phylogenetic weights. All filtered alignments derived from a unfiltered alignment were processed with identical phylogenies to remove any confounding effects of differing phylogenies. Note that while we employed FUBAR to assess positive selection for all simulation sets, we did not use PAML to infer positive selection for the largest set (158 sequences).

We then compared resulting positive-selection inferences for each alignment to its respective true alignment's $dN/dS$ values, given by Indelible during simulation, to assess performance accuracy. As residues may have been differently aligned relative to the true simulated alignment, we adopted a consensus method to compare the alignments we constructed to the true alignments. If at least 50\% of the residues present in a true alignment column were present in an inferred alignment column, we considered the true alignment column's $dN/dS$ as the true value for that inferred alignment column. We considered sites positively selected if the posterior probability of $(dN/dS>1)$ was $\geq0.9$.

Statistics were performed using Python and R. Linear modeling was conducted using the R package lme4 \citep{Bates2012}. We inferred effect magnitudes and significance and corrected for multiple testing using the R multcomp package's glht() function with default settings \citep{Hothorn2008}. All code used is available at https://github.com/clauswilke/alignment\underline{\hspace*{0.2cm}}filtering.


\section*{Acknowledgements}
This work was supported by ARO Grant W911NF-12-1-0390 and the National Institutes of Health (http://nih.gov/) grant R01 GM088344 to COW.

\bibliographystyle{MBE}
\bibliography{citations}	

\end{document}